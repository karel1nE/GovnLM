{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8539f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/karel1ne/nlp_course/.venv/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flash_attn_func\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/flash_attn/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.7.4.post1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     flash_attn_func,\n\u001b[1;32m      5\u001b[0m     flash_attn_kvpacked_func,\n\u001b[1;32m      6\u001b[0m     flash_attn_qkvpacked_func,\n\u001b[1;32m      7\u001b[0m     flash_attn_varlen_func,\n\u001b[1;32m      8\u001b[0m     flash_attn_varlen_kvpacked_func,\n\u001b[1;32m      9\u001b[0m     flash_attn_varlen_qkvpacked_func,\n\u001b[1;32m     10\u001b[0m     flash_attn_with_kvcache,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_triton_amd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interface_fa \u001b[38;5;28;01mas\u001b[39;00m flash_attn_gpu\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn_2_cuda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn_gpu\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmaybe_contiguous\u001b[39m(x):\n",
      "\u001b[0;31mImportError\u001b[0m: /home/karel1ne/nlp_course/.venv/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flash_attn import flash_attn_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b4448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn((16, 512, 16, 64), device='cuda')\n",
    "k = torch.randn((16, 512, 2, 64), device='cuda')\n",
    "v = torch.randn((16, 512, 2, 64), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce8254d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "FlashAttention only supports Ampere GPUs or newer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mflash_attn_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:1201\u001b[0m, in \u001b[0;36mflash_attn_func\u001b[0;34m(q, k, v, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_attn_probs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflash_attn_func\u001b[39m(\n\u001b[1;32m   1141\u001b[0m     q,\n\u001b[1;32m   1142\u001b[0m     k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     return_attn_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1152\u001b[0m ):\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"dropout_p should be set to 0.0 during evaluation\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m    Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;124;03m            pattern (negative means that location was dropped, nonnegative means it was kept).\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlashAttnFunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attn_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:839\u001b[0m, in \u001b[0;36mFlashAttnFunc.forward\u001b[0;34m(ctx, q, k, v, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_softmax, is_grad_enabled)\u001b[0m\n\u001b[1;32m    837\u001b[0m     k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(k, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m-\u001b[39m head_size_og \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m8\u001b[39m])\n\u001b[1;32m    838\u001b[0m     v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(v, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m-\u001b[39m head_size_og \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m8\u001b[39m])\n\u001b[0;32m--> 839\u001b[0m out_padded, softmax_lse, S_dmask, rng_state \u001b[38;5;241m=\u001b[39m \u001b[43m_wrapped_flash_attn_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size_left\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size_right\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_softmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_softmax\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_grad:\n\u001b[1;32m    853\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(q, k, v, out_padded, softmax_lse, rng_state)\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/torch/_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/torch/_library/autograd.py:113\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.autograd_impl\u001b[0;34m(keyset, *args, **keyword_only_args)\u001b[0m\n\u001b[1;32m    111\u001b[0m     result \u001b[38;5;241m=\u001b[39m Generated\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, Metadata(keyset, keyword_only_args))  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_no_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_only_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/torch/_library/autograd.py:40\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.forward_no_grad\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     38\u001b[0m keyset \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyset\n\u001b[1;32m     39\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyword_only_args\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_autograd_keyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/torch/_ops.py:721\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mredispatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch_boxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:324\u001b[0m, in \u001b[0;36mCustomOpDef.register_kernel.<locals>.inner.<locals>.backend_impl\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackend_impl\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# Checks the assumption that outputs cannot alias\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# inputs or other outputs.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     storages \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28mid\u001b[39m(tensor\u001b[38;5;241m.\u001b[39muntyped_storage())\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m iter_tensors(args, kwargs)\n\u001b[1;32m    322\u001b[0m     }\n\u001b[0;32m--> 324\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     tuple_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:367\u001b[0m, in \u001b[0;36mCustomOpDef.register_kernel.<locals>.inner.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp_course/.venv/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:96\u001b[0m, in \u001b[0;36m_flash_attn_forward\u001b[0;34m(q, k, v, dropout_p, softmax_scale, causal, window_size_left, window_size_right, softcap, alibi_slopes, return_softmax)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@_torch_custom_op_wrapper\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn::_flash_attn_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mutates_args\u001b[38;5;241m=\u001b[39m(), device_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_flash_attn_forward\u001b[39m(\n\u001b[1;32m     83\u001b[0m     q: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     return_softmax: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     94\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     95\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m [maybe_contiguous(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (q, k, v)]\n\u001b[0;32m---> 96\u001b[0m     out, softmax_lse, S_dmask, rng_state \u001b[38;5;241m=\u001b[39m \u001b[43mflash_attn_gpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size_left\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_softmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, softmax_lse, S_dmask, rng_state\n",
      "\u001b[0;31mRuntimeError\u001b[0m: FlashAttention only supports Ampere GPUs or newer."
     ]
    }
   ],
   "source": [
    "flash_attn_func(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792a31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GovnArgs, GovnLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61629c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = GovnArgs(vocab_size=32000, max_seq_len=512)\n",
    "model = GovnLM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77cf7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", split=\"train\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94f3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karel1ne/nlp_course/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = load_dataset(\"ashaba1in/small_openwebtext\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized['labels'] = tokenized['input_ids'].clone()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f4d30d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karel1ne/nlp_course/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = load_dataset('tokenized_smallwebtext').with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e2a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    tokenized_dataset['train'],\n",
    "    batch_size=args.max_batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a75dc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "with torch.no_grad():\n",
    "    print(batch['input_ids'].shape)\n",
    "    logits = model(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0e5236d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4194,\n",
       " 28733,\n",
       " 581,\n",
       " 28733,\n",
       " 3393,\n",
       " 1261,\n",
       " 28725,\n",
       " 382,\n",
       " 1022,\n",
       " 28710,\n",
       " 325,\n",
       " 28743,\n",
       " 11348,\n",
       " 28731,\n",
       " 1939,\n",
       " 8599,\n",
       " 364,\n",
       " 621,\n",
       " 13980,\n",
       " 28725,\n",
       " 1425,\n",
       " 372,\n",
       " 288,\n",
       " 297,\n",
       " 3358,\n",
       " 304,\n",
       " 26517,\n",
       " 2917,\n",
       " 438,\n",
       " 1411,\n",
       " 28725,\n",
       " 6758,\n",
       " 13500,\n",
       " 304,\n",
       " 26579,\n",
       " 2338,\n",
       " 1753,\n",
       " 477,\n",
       " 264,\n",
       " 1834,\n",
       " 6556,\n",
       " 8085,\n",
       " 2125,\n",
       " 1024,\n",
       " 264,\n",
       " 12172,\n",
       " 753,\n",
       " 5714,\n",
       " 1918,\n",
       " 25469,\n",
       " 25473,\n",
       " 272,\n",
       " 2698,\n",
       " 28725,\n",
       " 3677,\n",
       " 378,\n",
       " 403,\n",
       " 9059,\n",
       " 684,\n",
       " 4908,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 1014,\n",
       " 5161,\n",
       " 1749,\n",
       " 26656,\n",
       " 11762,\n",
       " 12195,\n",
       " 3198,\n",
       " 27092,\n",
       " 308,\n",
       " 3652,\n",
       " 28768,\n",
       " 339,\n",
       " 2480,\n",
       " 447,\n",
       " 28708,\n",
       " 390,\n",
       " 272,\n",
       " 865,\n",
       " 6676,\n",
       " 438,\n",
       " 272,\n",
       " 6556,\n",
       " 298,\n",
       " 625,\n",
       " 272,\n",
       " 6883,\n",
       " 1059,\n",
       " 272,\n",
       " 2125,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 28743,\n",
       " 11348,\n",
       " 12735,\n",
       " 5745,\n",
       " 28725,\n",
       " 2818,\n",
       " 356,\n",
       " 17256,\n",
       " 395,\n",
       " 741,\n",
       " 302,\n",
       " 272,\n",
       " 13500,\n",
       " 28725,\n",
       " 369,\n",
       " 272,\n",
       " 2969,\n",
       " 16419,\n",
       " 8346,\n",
       " 272,\n",
       " 12172,\n",
       " 753,\n",
       " 4205,\n",
       " 330,\n",
       " 313,\n",
       " 304,\n",
       " 10731,\n",
       " 6867,\n",
       " 298,\n",
       " 25469,\n",
       " 11931,\n",
       " 28723,\n",
       " 2993,\n",
       " 28725,\n",
       " 12172,\n",
       " 753,\n",
       " 11762,\n",
       " 23184,\n",
       " 10049,\n",
       " 2404,\n",
       " 930,\n",
       " 420,\n",
       " 1230,\n",
       " 28713,\n",
       " 28725,\n",
       " 264,\n",
       " 6676,\n",
       " 693,\n",
       " 403,\n",
       " 438,\n",
       " 272,\n",
       " 6556,\n",
       " 395,\n",
       " 28705,\n",
       " 28784,\n",
       " 28734,\n",
       " 12172,\n",
       " 753,\n",
       " 5714,\n",
       " 15817,\n",
       " 28725,\n",
       " 773,\n",
       " 378,\n",
       " 403,\n",
       " 516,\n",
       " 5161,\n",
       " 298,\n",
       " 3300,\n",
       " 272,\n",
       " 1918,\n",
       " 575,\n",
       " 354,\n",
       " 272,\n",
       " 2125,\n",
       " 28723,\n",
       " 420,\n",
       " 1230,\n",
       " 28713,\n",
       " 773,\n",
       " 400,\n",
       " 11939,\n",
       " 500,\n",
       " 28723,\n",
       " 28759,\n",
       " 28723,\n",
       " 4908,\n",
       " 15817,\n",
       " 298,\n",
       " 5084,\n",
       " 272,\n",
       " 6556,\n",
       " 22128,\n",
       " 28725,\n",
       " 562,\n",
       " 403,\n",
       " 2240,\n",
       " 369,\n",
       " 6405,\n",
       " 16409,\n",
       " 404,\n",
       " 682,\n",
       " 865,\n",
       " 347,\n",
       " 2358,\n",
       " 298,\n",
       " 25469,\n",
       " 11931,\n",
       " 272,\n",
       " 1918,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 2428,\n",
       " 773,\n",
       " 378,\n",
       " 403,\n",
       " 264,\n",
       " 345,\n",
       " 28707,\n",
       " 900,\n",
       " 5161,\n",
       " 28739,\n",
       " 562,\n",
       " 369,\n",
       " 400,\n",
       " 9008,\n",
       " 272,\n",
       " 500,\n",
       " 28723,\n",
       " 28759,\n",
       " 28723,\n",
       " 2405,\n",
       " 298,\n",
       " 25469,\n",
       " 11931,\n",
       " 1024,\n",
       " 264,\n",
       " 10435,\n",
       " 5714,\n",
       " 1918,\n",
       " 28725,\n",
       " 835,\n",
       " 438,\n",
       " 272,\n",
       " 6556,\n",
       " 395,\n",
       " 10435,\n",
       " 4908,\n",
       " 9164,\n",
       " 28725,\n",
       " 1749,\n",
       " 272,\n",
       " 3455,\n",
       " 8085,\n",
       " 8635,\n",
       " 28723,\n",
       " 415,\n",
       " 12172,\n",
       " 753,\n",
       " 1918,\n",
       " 4253,\n",
       " 8513,\n",
       " 3970,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 28777,\n",
       " 1230,\n",
       " 28713,\n",
       " 773,\n",
       " 272,\n",
       " 2969,\n",
       " 16419,\n",
       " 659,\n",
       " 5825,\n",
       " 298,\n",
       " 3084,\n",
       " 4908,\n",
       " 354,\n",
       " 8513,\n",
       " 2125,\n",
       " 28723,\n",
       " 415,\n",
       " 1918,\n",
       " 659,\n",
       " 11939,\n",
       " 272,\n",
       " 12172,\n",
       " 753,\n",
       " 3058,\n",
       " 298,\n",
       " 4080,\n",
       " 871,\n",
       " 1216,\n",
       " 10746,\n",
       " 354,\n",
       " 272,\n",
       " 1834,\n",
       " 6556,\n",
       " 28725,\n",
       " 690,\n",
       " 420,\n",
       " 1230,\n",
       " 28713,\n",
       " 28190,\n",
       " 298,\n",
       " 12688,\n",
       " 3909,\n",
       " 7729,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 1146,\n",
       " 766,\n",
       " 3202,\n",
       " 298,\n",
       " 272,\n",
       " 26656,\n",
       " 2264,\n",
       " 369,\n",
       " 2480,\n",
       " 447,\n",
       " 28708,\n",
       " 403,\n",
       " 272,\n",
       " 865,\n",
       " 6676,\n",
       " 1749,\n",
       " 438,\n",
       " 272,\n",
       " 4194,\n",
       " 28733,\n",
       " 581,\n",
       " 28733,\n",
       " 3393,\n",
       " 1261,\n",
       " 1834,\n",
       " 6556,\n",
       " 28725,\n",
       " 500,\n",
       " 28723,\n",
       " 28759,\n",
       " 28723,\n",
       " 23287,\n",
       " 1294,\n",
       " 7204,\n",
       " 418,\n",
       " 274,\n",
       " 361,\n",
       " 4845,\n",
       " 773,\n",
       " 8513,\n",
       " 369,\n",
       " 272,\n",
       " 1526,\n",
       " 2187,\n",
       " 28742,\n",
       " 28713,\n",
       " 7023,\n",
       " 297,\n",
       " 382,\n",
       " 1022,\n",
       " 28710,\n",
       " 863,\n",
       " 459,\n",
       " 1745,\n",
       " 707,\n",
       " 5714,\n",
       " 1918,\n",
       " 298,\n",
       " 3530,\n",
       " 28723,\n",
       " 1047,\n",
       " 272,\n",
       " 1918,\n",
       " 1749,\n",
       " 28725,\n",
       " 378,\n",
       " 403,\n",
       " 438,\n",
       " 272,\n",
       " 2159,\n",
       " 302,\n",
       " 652,\n",
       " 1216,\n",
       " 6666,\n",
       " 28725,\n",
       " 400,\n",
       " 773,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 3565,\n",
       " 8025,\n",
       " 10495,\n",
       " 895,\n",
       " 28725,\n",
       " 272,\n",
       " 500,\n",
       " 28723,\n",
       " 28759,\n",
       " 28723,\n",
       " 13892,\n",
       " 16916,\n",
       " 2952,\n",
       " 354,\n",
       " 6405,\n",
       " 26840,\n",
       " 6933,\n",
       " 28725,\n",
       " 2240,\n",
       " 25875,\n",
       " 2062,\n",
       " 369,\n",
       " 1862,\n",
       " 4908,\n",
       " 9164,\n",
       " 24328,\n",
       " 272,\n",
       " 2870,\n",
       " 28716,\n",
       " 1795,\n",
       " 6556,\n",
       " 15426,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 28739,\n",
       " 1313,\n",
       " 3969,\n",
       " 369,\n",
       " 478,\n",
       " 28742,\n",
       " 333,\n",
       " 3364,\n",
       " 741,\n",
       " 7959,\n",
       " 297,\n",
       " 272,\n",
       " 5611,\n",
       " 4077,\n",
       " 369,\n",
       " 272,\n",
       " 2969,\n",
       " 16419,\n",
       " 2261,\n",
       " 442,\n",
       " 7207,\n",
       " 741,\n",
       " 5714,\n",
       " 6941,\n",
       " 298,\n",
       " 459,\n",
       " 771,\n",
       " 707,\n",
       " 680,\n",
       " 297,\n",
       " 741,\n",
       " 24629,\n",
       " 1939,\n",
       " 369,\n",
       " 349,\n",
       " 459,\n",
       " 1132,\n",
       " 28725,\n",
       " 369,\n",
       " 349,\n",
       " 4716,\n",
       " 521,\n",
       " 3307,\n",
       " 862,\n",
       " 10495,\n",
       " 895,\n",
       " 773,\n",
       " 8513,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 28743,\n",
       " 11348,\n",
       " 3798,\n",
       " 477,\n",
       " 272,\n",
       " 6337,\n",
       " 8085,\n",
       " 2125,\n",
       " 4370,\n",
       " 272,\n",
       " 12172,\n",
       " 753,\n",
       " 1918,\n",
       " 26894,\n",
       " 582,\n",
       " 871,\n",
       " 13679,\n",
       " 304,\n",
       " 6285,\n",
       " 395,\n",
       " 396,\n",
       " 25514,\n",
       " 302,\n",
       " 5045,\n",
       " 28733,\n",
       " 2805,\n",
       " 2243,\n",
       " 286,\n",
       " 500,\n",
       " 28723,\n",
       " 28759,\n",
       " 28723,\n",
       " 6405,\n",
       " 16409,\n",
       " 404,\n",
       " 297,\n",
       " 10727,\n",
       " 22988]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b736d5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c14de95c6f4248a5cb73f3060a4813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/14 shards):   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk('tokenized_smallwebtext')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df0027e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GovnArgs, GovnLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd240a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karel1ne/nlp_course/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92298a81dd404eeda1a4d707b97dc34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader)):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 36\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     37\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m input_ids[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(54)\n",
    "device = 'cuda:0'\n",
    "args = GovnArgs(\n",
    "    dim=1024,\n",
    "    n_layers=36,\n",
    "    n_heads=16,\n",
    "    vocab_size=32000,\n",
    "    feed_forward_hidden_dim=4096,\n",
    "    norm_eps=1e-6,\n",
    "    max_batch_size=16,\n",
    "    max_seq_len=1024\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", split=\"train\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset('tokenized_smallwebtext').with_format('torch')\n",
    "dataloader = DataLoader(\n",
    "    dataset['train'],\n",
    "    batch_size=args.max_batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "log_dir = \"runs/GovnLM-0.5B\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "model = GovnLM(args).to(device=device, dtype=torch.bfloat16)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "sheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(dataloader))\n",
    "\n",
    "model.train()\n",
    "best_loss = float('inf')\n",
    "for step, batch in tqdm(enumerate(dataloader)):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    inputs = input_ids[:, :-1]  \n",
    "    labels = labels[:, 1:]\n",
    "    \n",
    "    logits = model(inputs)\n",
    "    \n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(\n",
    "        logits.view(-1, args.vocab_size),\n",
    "        labels.reshape(-1)\n",
    "    )\n",
    "            \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    sheduler.step()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        writer.add_scalar(f\"loss\", loss.item(), step * args.max_batch_size * args.max_seq_len)\n",
    "        writer.add_scalar(f\"lr\", sheduler.get_last_lr()[0], step * args.max_batch_size * args.max_seq_len)\n",
    "    if loss.item() < best_loss and step > 5000:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), f\"model_best.pt\")\n",
    "    if step % 5000 == 0:\n",
    "        torch.save(model.state_dict(), f\"model_{step}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c881eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    tokenized_dataset['train'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bba1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "log_dir = \"runs/GovnLM-0.5B\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98dd1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "args = GovnArgs(\n",
    "    dim=1024,\n",
    "    n_layers=36,\n",
    "    n_heads=16,\n",
    "    vocab_size=32000,\n",
    "    feed_forward_hidden_dim=4096,\n",
    "    norm_eps=1e-6,\n",
    "    max_batch_size=16,\n",
    "    max_seq_len=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c6fdc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62500"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "494819ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db0bf78addb484da3ed2320ed04ec8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de4fae85391406f9933be35ab363256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eba67d5bb0f4a5dae3ab6d9295ad297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cd2e10d70847aea158378f1ac2bf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac38240fc34b40b7a6b3d9bc9de4e121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d13120e4554bf9b0a955b4d83199cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6638138c2dbc4224895d85784f5a8d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fb4b2ac7ba49178ad653bc436ddcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55b9eb8d0a4496a96f049e0d2194a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ab1425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027ad07e835c4d9f8c12b64ba3479156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d538e36a7d4cc3b53732510ef47272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9338497a63ed4d968cde4d99ff8d17dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acc5d2c461b413cb3413661f6d31848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef9ec3ff96945d789fdb7267e74a4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4209f9f686ac4e799e8bcfaa378d139a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a58d7b77684e84b13fd178e99f9480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9538bc2580124003ad9e111aee805c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets.save_to_disk('glue-mrpc')\n",
    "datasets = load_dataset('glue-mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c717d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeac7d9d675b48499dbe58083f271758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     18\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fct(\n\u001b[1;32m     20\u001b[0m     logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mvocab_size),\n\u001b[1;32m     21\u001b[0m     labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = GovnLM(args).to(device=device, dtype=torch.bfloat16)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "sheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(dataloader))\n",
    "model.train()\n",
    "best_loss = float('inf')\n",
    "for step, batch in tqdm(enumerate(dataloader)):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    inputs = input_ids[:, :-1]  \n",
    "    labels = labels[:, 1:]\n",
    "    \n",
    "    logits = model(inputs)\n",
    "    \n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(\n",
    "        logits.view(-1, args.vocab_size),\n",
    "        labels.reshape(-1)\n",
    "    )\n",
    "    \n",
    "    perplexity = torch.exp(loss).item()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    sheduler.step()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        writer.add_scalar(f\"Batch{batch_size}/loss\", loss.item(), step)\n",
    "        writer.add_scalar(f\"Batch{batch_size}/perplexity\", perplexity, step)        \n",
    "    if step % 200 == 0:\n",
    "        torch.save(model.state_dict(), f\"model_{step}.pt\")\n",
    "    if loss.item() < best_loss and step > 2000:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), f\"model_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd460c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649501a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2397210/545879141.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"model_best.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"model_best.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4daa1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "Question:\n",
    "An important source of information on the credit rating of retail businesses is\n",
    "Options:\n",
    "(A) the Retail Merchants Association.\n",
    "(B) the local chamber of commerce.\n",
    "(C) Dun & Bradstreet, Inc.\n",
    "(D) the United States Retail Credit Association.\n",
    "Correct option:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14022d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bd0f7efd5347dbaa8942780cdbcc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "An important source of information on the credit rating of retail businesses is\n",
      "Options:\n",
      "(A) the Retail Merchants Association.\n",
      "(B) the local chamber of commerce.\n",
      "(C) Dun & Bradstreet, Inc.\n",
      "(D) the United States Retail Credit Association.\n",
      "Correct option:\n",
      "\n",
      "(A) the credit rating of\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    for _ in tqdm(range(7)):  \n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        probs = torch.softmax(next_token_logits * 10, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "        \n",
    "        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dca56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'California is the richest state in America.\\n\\nIn a state that includes the U.S. Constitution, the state of Minnesota on the state’s second-largest federal law, has been on the rise in the past decade and has been largely associated with the state the state.\\n\\nThe state’s legislature has made a state-of-the-art plan to eliminate all the state’s state tax credits and state-of-the-art ordinance rules, with many states approving a $2.5-billion amendment allowing a state of the state to use a state-state solution of state-controlled state-of-the-art state laws.\\n\\nThat’s also the first in a state that needs more state-of-the-art legislation to change its bill for nonprofits and state governments under the age 10-21 law.\\n\\nBut while most county groups, it comes back and states cover big provisions seeking closer control-energy, the state-owned state package for states that have a corporate job maximise at one state level.\\n\\nOver the last decade, one whereby Fergus Weakuran, neither the governor nor to the National'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6500e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
